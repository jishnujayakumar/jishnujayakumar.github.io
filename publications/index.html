<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Jishnu Jaykumar Padalunkal</title> <meta name="author" content="Jishnu Jaykumar Padalunkal"> <meta name="description" content="&lt;a href='https://scholar.google.com/citations?user=08esT74AAAAJ&amp;hl=en&amp;&amp;sortby=pubdate' target='_blank'&gt;&lt;b class='google-scholar-link'&gt;Google Scholar&lt;/b&gt;&lt;/a&gt; | * denotes equal contribution and joint lead authorship."> <meta name="keywords" content="jishnu jaykumar padalunkal, phd student, utdallas, nvidia, iit-kgp, iisc, iiit vadodara"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jishnu </span>Jaykumar Padalunkal</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/presentations">Presentations</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching">Teaching</a> </li> <li class="nav-item"> <a class="nav-link" href="https://drive.google.com/file/d/1VUsrI9BKeB4lmhxnfEi8CYO9jLzfR5vc" target="_blank">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"><a href="https://scholar.google.com/citations?user=08esT74AAAAJ&amp;hl=en&amp;&amp;sortby=pubdate" target="_blank"><b class="google-scholar-link">Google Scholar</b></a> | * denotes equal contribution and joint lead authorship.</p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="https://irvlutd.github.io/iTeach/assets/images/iteach/iteach-overview.webp"></div> <div id="padalunkal2024iteach" class="col-sm-8"> <div class="title">iTeach: Interactive Teaching for Robot Perception using Mixed Reality</div> <div class="author"> <em>Jishnu Jaykumar P</em>, <a href="https://labs.utdallas.edu/irvl/people/">Cole Salvato</a>, <a href="https://labs.utdallas.edu/irvl/people/">Vinaya Bomnale</a>, <a href="https://labs.utdallas.edu/irvl/people/">Jikai Wang</a>, and <a href="https://scholar.google.com/citations?user=9cZUlEYAAAAJ">Yu Xiang</a> </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://arxiv.org/pdf/2410.09072" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/IRVLUTD/iTeach" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://irvlutd.github.io/iTeach" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We introduce iTeach, a Mixed Reality (MR) framework to improve robot perception through real-time interactive teaching. By allowing human instructors to dynamically label robot RGB data, iTeach improves both the accuracy and adaptability of robot perception to new scenarios. The framework supports on-the-fly data collection and labeling, enhancing model performance, and generalization. Applied to door and handle detection for household tasks, iTeach integrates a HoloLens app with an interactive YOLO model. Furthermore, we introduce the IRVLUTD DoorHandle dataset. DH-YOLO, our efficient detection model, significantly enhances the accuracy and efficiency of door and handle detection, highlighting the potential of MR to make robotic systems more capable and adaptive in real-world environments. The project page is available at https://irvlutd.github.io/iTeach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">padalunkal2024iteach</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{iTeach: Interactive Teaching for Robot Perception using Mixed Reality}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{P, Jishnu Jaykumar and Salvato, Cole and Bomnale, Vinaya and Wang, Jikai and Xiang, Yu}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="https://irvlutd.github.io/NIDSNet/assets/images/fw0.png"></div> <div id="lu2024adapting" class="col-sm-8"> <div class="title">Adapting Pre-Trained Vision Models for Novel Instance Detection and Segmentation</div> <div class="author"> <a href="https://youngsean.github.io/">Yangxiao Lu</a>, <em>Jishnu Jaykumar P</em>, <a href="https://yunhuiguo.github.io/">Yunhui Guo</a>, Nicholas Ruozzi, and <a href="https://scholar.google.com/citations?user=9cZUlEYAAAAJ">Yu Xiang</a> </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://arxiv.org/pdf/2405.17859.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/YoungSean/NIDS-Net" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://irvlutd.github.io/NIDSNet" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Novel Instance Detection and Segmentation (NIDS) aims at detecting and segmenting novel object instances given a few examples of each instance. We propose a unified framework (NIDS-Net) comprising object proposal generation, embedding creation for both instance templates and proposal regions, and embedding matching for instance label assignment. Leveraging recent advancements in large vision methods, we utilize the Grounding DINO and Segment Anything Model (SAM) to obtain object proposals with accurate bounding boxes and masks. Central to our approach is the generation of high-quality instance embeddings. We utilize foreground feature averages of patch embeddings from the DINOv2 ViT backbone, followed by refinement through a weight adapter mechanism that we introduce. We show experimentally that our weight adapter can adjust the embeddings locally within their feature space and effectively limit overfitting. This methodology enables a straightforward matching strategy, resulting in significant performance gains. Our framework surpasses current state-of-the-art methods, demonstrating notable improvements of 22.3, 46.2, 10.3, and 24.0 in average precision (AP) across four detection datasets. In instance segmentation tasks on seven core datasets of the BOP challenge, our method outperforms the top RGB methods by 3.6 AP and remains competitive with the best RGB-D method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lu2024adapting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adapting Pre-Trained Vision Models for Novel Instance Detection and Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lu, Yangxiao and P, Jishnu Jaykumar and Guo, Yunhui and Ruozzi, Nicholas and Xiang, Yu}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_thumbnails/real-world.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_thumbnails/real-world.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_thumbnails/real-world.gif-1400.webp"></source> <img src="/assets/img/publication_thumbnails/real-world.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="real-world.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="padalunkal2024protoclip" class="col-sm-8"> <div class="title">Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning</div> <div class="author"> <em>Jishnu Jaykumar P</em>, <a href="https://scholar.google.com/citations?user=wPO_ZgUAAAAJ">Kamalesh Palanisamy</a>, <a href="https://scholar.google.com/citations?user=48Y9F-YAAAAJ">Yu-Wei Chao</a>, <a href="https://scholar.google.com/citations?user=R-lKQqkAAAAJ&amp;hl=en">Xinya Du</a>, and <a href="https://scholar.google.com/citations?user=9cZUlEYAAAAJ">Yu Xiang</a> </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://arxiv.org/pdf/2307.03073.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/IRVLUTD/Proto-CLIP" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="/assets/pdf/research/phd/Proto-CLIP/Proto_CLIP_IROS_2024_Poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a> <a href="/assets/pdf/research/phd/Proto-CLIP/proto-clip-graphical-abstract.jpg" class="btn btn-sm z-depth-0" role="button" target="_blank">Graphical Abstract</a> <a href="/assets/pdf/research/phd/Proto-CLIP/IROS24_Proto-CLIP.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://irvlutd.github.io/Proto-CLIP" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/IROS58592.2024.10801660"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/IROS58592.2024.10801660" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We propose a novel framework for few-shot learning by leveraging large-scale vision-language models such as CLIP. Motivated by the unimodal prototypical networks for few-shot learning, we introduce PROTO-CLIP that utilizes image prototypes and text prototypes for few-shot learning. Specifically, PROTO-CLIP adapts the image encoder and text encoder in CLIP in a joint fashion using few-shot examples. The two encoders are used to compute prototypes of image classes for classification. During adaptation, we propose aligning the image and text prototypes of corresponding classes. Such a proposed alignment is beneficial for few-shot classification due to the contributions from both types of prototypes. We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning as well as in the real world for robot perception.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">padalunkal2024protoclip</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{P, Jishnu Jaykumar and Palanisamy, Kamalesh and Chao, Yu-Wei and Du, Xinya and Xiang, Yu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning}}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Training;Representation learning;Adaptation models;Three-dimensional displays;Prototypes;Benchmark testing;Object recognition;Few shot learning;Intelligent robots}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS58592.2024.10801660}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2594-2601}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_thumbnails/scene-replica.webp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_thumbnails/scene-replica.webp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_thumbnails/scene-replica.webp-1400.webp"></source> <img src="/assets/img/publication_thumbnails/scene-replica.webp" class="preview z-depth-1 rounded" width="auto" height="auto" alt="scene-replica.webp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="khargonkar2024scenereplica" class="col-sm-8"> <div class="title">SceneReplica: Benchmarking Real-World Robot Manipulation by Creating Replicable Scenes</div> <div class="author"> <a href="https://kninad.github.io/">Ninad Khargonkar</a>, <a href="https://labs.utdallas.edu/irvl/people/">Sai Haneesh Allu</a>, <a href="https://youngsean.github.io/">Yangxiao Lu</a>, <em>Jishnu Jaykumar P</em>, <a href="https://scholar.google.com/citations?user=4yki88YAAAAJ&amp;hl=en">Balakrishnan Prabhakaran</a>, and <a href="https://scholar.google.com/citations?user=9cZUlEYAAAAJ">Yu Xiang</a> </div> <div class="periodical"> <em>In 2024 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10610180" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/IRVLUTD/SceneReplica" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://irvlutd.github.io/SceneReplica/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/ICRA57147.2024.10610180"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICRA57147.2024.10610180" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on the task of pick-and-place. Our benchmark uses the YCB objects, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible for researchers and practitioners. We also provide our experimental results and analyses for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms for object perception, grasping planning and motion planning are evaluated. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">khargonkar2024scenereplica</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khargonkar, Ninad and Allu, Sai Haneesh and Lu, Yangxiao and P, Jishnu Jaykumar and Prabhakaran, Balakrishnan and Xiang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{SceneReplica: Benchmarking Real-World Robot Manipulation by Creating Replicable Scenes}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8258-8264}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Analytical models;Focusing;Grasping;Benchmark testing;Planning;Task analysis;Robots}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA57147.2024.10610180}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_thumbnails/fewsol.webp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_thumbnails/fewsol.webp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_thumbnails/fewsol.webp-1400.webp"></source> <img src="/assets/img/publication_thumbnails/fewsol.webp" class="preview z-depth-1 rounded" width="auto" height="auto" alt="fewsol.webp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="padalunkal2023fewsol" class="col-sm-8"> <div class="title">FewSOL: A Dataset for Few-Shot Object Learning in Robotic Environments</div> <div class="author"> <em>Jishnu Jaykumar P</em>, <a href="https://scholar.google.com/citations?user=48Y9F-YAAAAJ">Yu-Wei Chao</a>, and <a href="https://scholar.google.com/citations?user=9cZUlEYAAAAJ">Yu Xiang</a> </div> <div class="periodical"> <em>In 2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://arxiv.org/pdf/2207.03333.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/IRVLUTD/few-shot-dataset" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="/assets/pdf/research/phd/FewSOL/posters/ICRA2023_FewSOL_Poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a> <a href="/assets/pdf/research/phd/FewSOL/FewSOL-ICRA23-Graphical-Abstract.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Graphical Abstract</a> <a href="https://irvlutd.github.io/FewSOL" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/ICRA48891.2023.10161143"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICRA48891.2023.10161143" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We introduce the Few-Shot Object Learning (FEWSOL) dataset for object recognition with a few images per object. We captured 336 real-world objects with 9 RGB-D images per object from different views. Fewsol has object segmentation masks, poses, and attributes. In addition, synthetic images generated using 330 3D object models are used to augment the dataset. We investigated (i) few-shot object classification and (ii) joint object segmentation and few-shot classification with state-of-the-art methods for few-shot learning and meta-learning using our dataset. The evaluation results show the presence of a large margin to be improved for few-shot object classification in robotic environments, and our dataset can be used to study and enhance few-shot object recognition for robot perception. Dataset and code available at https://irvlutd.github.io/FewSOL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">padalunkal2023fewsol</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{FewSOL: A Dataset for Few-Shot Object Learning in Robotic Environments}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{P, Jishnu Jaykumar and Chao, Yu-Wei and Xiang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48891.2023.10161143}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9140-9146}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_thumbnails/seq-analysis.webp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_thumbnails/seq-analysis.webp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_thumbnails/seq-analysis.webp-1400.webp"></source> <img src="/assets/img/publication_thumbnails/seq-analysis.webp" class="preview z-depth-1 rounded" width="auto" height="auto" alt="seq-analysis.webp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chattopadhyay2023sequential" class="col-sm-8"> <div class="title">A sequential approach for noninferiority or equivalence of a linear contrast under cost constraints.</div> <div class="author"> <a href="https://scholar.google.com/citations?user=PTS3aeAAAAAJ&amp;hl=en&amp;oi=ao">Bhargab Chattopadhyay</a>, <a href="https://scholar.google.co.in/citations?user=NZjB-lUAAAAJ&amp;hl=en">Tathagata Bandyopadhyay</a>, <a href="https://scholar.google.com/citations?user=G9JORkUAAAAJ&amp;hl=en&amp;oi=ao">Ken Kelley</a>, and Jishnu J Padalunkal</div> <div class="periodical"> <em>Psychological Methods</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1037/met0000570"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1037/met0000570" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Planning an appropriate sample size for a study involves considering several issues. Two important considerations are cost constraints and variability inherent in the population from which data will be sampled. Methodologists have developed sample size planning methods for two or more populations when testing for equivalence or noninferiority/superiority for a linear contrast of population means. Additionally, cost constraints and variance heterogeneity among populations have also been considered. We extend these methods by developing a theory for sequential procedures for testing the equivalence or noninferiority/superiority for a linear contrast of population means under cost constraints, which we prove to effectively utilize the allocated resources. Our method, due to the sequential framework, does not require prespecified values of unknown population variance(s), something that is historically an impediment to designing studies. Importantly, our method does not require an assumption of a specific type of distribution of the data in the relevant population from which the observations are sampled, as we make our developments in a data distribution-free context. We provide an illustrative example to show how the implementation of the proposed approach can be useful in applied research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chattopadhyay2023sequential</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A sequential approach for noninferiority or equivalence of a linear contrast under cost constraints.}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chattopadhyay, Bhargab and Bandyopadhyay, Tathagata and Kelley, Ken and Padalunkal, Jishnu J}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Psychological Methods}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{American Psychological Association}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1037/met0000570}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_thumbnails/re-embed-kgqa.webp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_thumbnails/re-embed-kgqa.webp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_thumbnails/re-embed-kgqa.webp-1400.webp"></source> <img src="/assets/img/publication_thumbnails/re-embed-kgqa.webp" class="preview z-depth-1 rounded" width="auto" height="auto" alt="re-embed-kgqa.webp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="P2021" class="col-sm-8"> <div class="title">[Re] Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings</div> <div class="author"> <em>Jishnu Jaykumar P</em>, and <a href="https://scholar.google.co.in/citations?hl=en&amp;user=m2QUCyYAAAAJ">Ashish Sardana</a> </div> <div class="periodical"> May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://zenodo.org/record/4834942/files/article.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jishnujayakumar/MLRC2020-EmbedKGQA" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://rescience.github.io/bibliography/P_2021.html" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.5281/ZENODO.4834942"></span> <span class="__dimensions_badge_embed__" data-doi="10.5281/ZENODO.4834942" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Our work consists of four parts: (1) Reproducing results from Saxena et al. [2020] (2) Adding more experiments by replacing the knowledge graph embedding method (3) and exploring the question embedding method using various transformer models (4) Verifying the importance of Relation Matching (RM) module. Based on the code shared by the authors, we have reproduced the results for the EmbedKGQA method. We have not purposely performed relation matching to validate point-4. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">P2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{[{Re}] {Improving} {Multi}-hop {Question} {Answering} over {Knowledge} {Graphs} using {Knowledge} {Base} {Embeddings}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{P, Jishnu Jaykumar and Sardana, Ashish}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ReScience C}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ReScience C}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://zenodo.org/record/4834942}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/ZENODO.4834942}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{#15}</span><span class="p">,</span>
  <span class="na">review_url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=VFAwCMdWY7}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">type</span> <span class="p">=</span> <span class="s">{Replication}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{Python}</span><span class="p">,</span>
  <span class="na">domain</span> <span class="p">=</span> <span class="s">{ML Reproducibility Challenge 2020}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{knowledge graph, embeddings, multi-hop, question-answering, deep learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_thumbnails/edge-detect.webp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_thumbnails/edge-detect.webp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_thumbnails/edge-detect.webp-1400.webp"></source> <img src="/assets/img/publication_thumbnails/edge-detect.webp" class="preview z-depth-1 rounded" width="auto" height="auto" alt="edge-detect.webp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="singh2021edgedetect" class="col-sm-8"> <div class="title">Edge-Detect: Edge-Centric Network Intrusion Detection using Deep Neural Network</div> <div class="author"> <a href="https://scholar.google.co.in/citations?hl=en&amp;user=TUX6cg8AAAAJ">Praneet Singh</a>, <em>Jishnu Jaykumar P</em>, <a href="https://in.linkedin.com/in/akhil-pankaj">Akhil Pankaj</a>, and <a href="https://scholar.google.co.in/citations?hl=en&amp;user=EZP8-BMAAAAJ">Reshmi Mitra</a> </div> <div class="periodical"> <em>In 2021 IEEE 18th Annual Consumer Communications &amp; Networking Conference (CCNC)</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://ieeexplore.ieee.org/document/9369469" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/racsa-lab/Edge-Detect" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/CCNC49032.2021.9369469"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/CCNC49032.2021.9369469" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Edge nodes are crucial for detection against multitudes of cyber attacks on Internet-of-Things endpoints and is set to become part of a multi-billion industry. The resource constraints in this novel network infrastructure tier constricts the deployment of existing Network Intrusion Detection System with Deep Learning models (DLM). We address this issue by developing a novel light, fast and accurate ‘Edge-Detect’ model, which detects Distributed Denial of Service attack on edge nodes using DLM techniques. Our model can work within resource restrictions i.e. low power, memory and processing capabilities, to produce accurate results at a meaningful pace. It is built by creating layers of Long Short-Term Memory or Gated Recurrent Unit based cells, which are known for their excellent representation of sequential data. We designed a practical data science pipeline with Recurring Neural Network to learn from the network packet behavior in order to identify whether it is normal or attack-oriented. The model evaluation is from deployment on actual edge node represented by Raspberry Pi using current cybersecurity dataset (UNSW2015). Our results demonstrate that in comparison to conventional DLM techniques, our model maintains a high testing accuracy of  99% even with lower resource utilization in terms of cpu and memory. In addition, it is nearly 3 times smaller in size than the state-of-art model and yet requires a much lower testing time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">singh2021edgedetect</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Singh, Praneet and P, Jishnu Jaykumar and Pankaj, Akhil and Mitra, Reshmi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE 18th Annual Consumer Communications &amp; Networking Conference (CCNC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Edge-Detect: Edge-Centric Network Intrusion Detection using Deep Neural Network}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-6}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CCNC49032.2021.9369469}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_thumbnails/mv-tractus.webp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_thumbnails/mv-tractus.webp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_thumbnails/mv-tractus.webp-1400.webp"></source> <img src="/assets/img/publication_thumbnails/mv-tractus.webp" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mv-tractus.webp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="p_mvtractus_2018" class="col-sm-8"> <div class="title">MV-Tractus: A simple tool to extract motion vectors from H264 encoded video sources</div> <div class="author"> <em>Jishnu P</em>, and <a href="https://scholar.google.co.in/citations?hl=en&amp;user=TUX6cg8AAAAJ">Praneet Singh</a> </div> <div class="periodical"> Oct 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://github.com/jishnujayakumar/MV-Tractus" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://doi.org/10.5281/zenodo.4422613" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.5281/zenodo.4422613"></span> <span class="__dimensions_badge_embed__" data-doi="10.5281/zenodo.4422613" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@software</span><span class="p">{</span><span class="nl">p_mvtractus_2018</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Zenodo}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{P, Jishnu and Singh, Praneet}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Zenodo}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">version</span> <span class="p">=</span> <span class="s">{2.0}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.4422613}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_thumbnails/va-4-tm.webp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_thumbnails/va-4-tm.webp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_thumbnails/va-4-tm.webp-1400.webp"></source> <img src="/assets/img/publication_thumbnails/va-4-tm.webp" class="preview z-depth-1 rounded" width="auto" height="auto" alt="va-4-tm.webp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cyphyss18va" class="col-sm-8"> <div class="title">Video Analytics using YOLO and DeepSORT for Traffic Modelling</div> <div class="author"> <a href="https://scholar.google.co.in/citations?user=OftxRCEAAAAJ&amp;hl=en">Raghu Krishnapuram</a>, <a href="https://in.linkedin.com/in/ayush-sawarni">Ayush Sawarni</a>, <a href="https://www.linkedin.com/in/nishalpereira">Nishal Pereira</a>, <em>Jishnu P</em>, <a href="https://scholar.google.co.in/citations?hl=en&amp;user=EjDp6ogAAAAJ">Prajwal Rao</a>, <a href="https://scholar.google.co.in/citations?hl=en&amp;user=TUX6cg8AAAAJ">Praneet Singh</a>, <a href="https://in.linkedin.com/in/abhay-sharma-2b319316">Abhay Sharma</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Soma Biswas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>2nd Cyber-Physical Systems Symposium (CyPhySS), Indian Institute of Science, Bangalore</em>, Oct 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://drive.google.com/file/d/1lQV1N-gqOIh_1S3yiOnNDjykZOmqLfil" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a> <a href="http://www.rbccps.org/cyphyss2018/posters-and-demos/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <center> <div class="container mt-0"> © Copyright 2025 Jishnu Jaykumar Padalunkal. Theme: <a href="https://github.com/alshedivat/al-folio" target="_blank">Al-Folio</a>. Last updated: September 17, 2025. </div> </center> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-GDXDM8QC54"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-GDXDM8QC54");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>